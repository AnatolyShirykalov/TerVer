\section{Лекция 6}
Мы дожили до такого момента, когда нам придётся заниматься математикой. Вспомнили, что такое интеграл Лебега и определили мат. ожидание
\[
  \E\xi = \int\limits_\Omega \xi(\omega) \P(d\omega) = \lim\limits_{n\to\infty} \rY k{-\infty} \frac{k}{n} \P\left\{ \omega\colon \frac kn\le \xi(\omega) <\frac{k+1}n \right\}.
\]
Но считать такой предел почти невозможно, нужно перейти к каким-то другим понятиям.

Пусть есть случайная величина $\xi = \xi(\omega)$ ($\xi^{-1}(B)\in \mathfrak B$), есть $f(x)$ измеримая по Борелю (то есть аргументы $x\in\R$ действительные, значения $f(x)\in\R$ тоже). Придумать действительную неизмеримую по Борелю функцию достаточно сложно. Кто забыл, $f^{-1}(B)$ борелевское, если $B$ борелевское, "--- выполнение этого означает измеримость. В наших предположениях $f(\xi) = f\big(\xi(\omega)\big) = \eta(\omega)$. Тогда
\[
  \Big\{\omega\colon f\big(\xi(\omega)\big)\in B\Big\} = \big\{\omega\colon \xi(\omega)\in f^{-1} (B) \big\}\in \mathfrak B.
\]
И теорема вот какая:
\[
  \E f(\xi) = \int\limits_{-\infty}^{+\infty} f(x)\,\mu_\xi(dx),\qquad
  \mu_\xi(B) = \P\big\{\omega\colon \xi(\omega)\in B\big\} = \P\big\{\xi^{-1}(B)\big\}.
\]
% тут картинка со всякими участвующими прообразами-облаками, отображения обозначены стрелочками
Новая мера $\mu_\xi(B)$ тоже $\sigma$-аддитивна. Она носит название распределения вероятности случайной величины $\xi$.

Очень редко задаются в приложениях зависимости $\xi(\omega)$, а вот распределения вероятностей встречаются довольно часто. Так вот что утверждается
\[
  \E f(\xi) = \int\limits_{\R^1} f(x)\mu_\xi(dx).
\]
Обратите внимание на логику. $\mu_\xi$ "--- это вероятностная мера. Интеграл Лебега для неё заново определять нам не нужно. Математическое ожидание функции от случайной величины равно интегралу от этой функции по распределению вероятности случайной величины.

Многие студенты в этой формуле потают, пишут, например, $f(\omega)$, хотя $f$ "--- функция вещественного аргумента и в этом суть.

Такой интеграл тоже никто вычислять не умеет. Но это наш первый шаг. В некоторых случаях его вычислить всё-таки можно.

\begin{Proof}
  Доказательство вот к чему сводится.
\begin{multline*}
  \E f(\xi) = \int\limits_{\Omega} f\big( \xi(\omega)\big) \P(d\omega) = \lim\limits_{n\to\infty} \rY k{-\infty}\frac kn\P\left\{ \omega\colon \frac ln \le \xi(\omega) < \frac{k+1}n \right\} = 
\\= \lim\limits_{n\to\infty} \rY k{-\infty} \frac kn \P\left\{  f\big(\xi(\omega)\big)\in \left[ \frac kn,\frac{k+1}n \right) \right\} = 
    \lim\limits_{n\to\infty} \rY k{-\infty} \frac kn \P\left\{ \xi(\omega)\in f^{-1} \left[ \frac kn,\frac{k+1}n \right) \right\} = \\
   \lim\limits_{n\to\infty} \rY k{-\infty} \frac kn \mu_\xi \left\{ x\colon f(x)\in \left[ \frac kn,\frac{k+1}n \right) \right\} 
  = \int\limits_{\R^1} f(x)\,\mu_\xi(dx).
\end{multline*}
\end{Proof}

Случайнвя величина $\xi = \xi(\omega)$, её распределение вероятностей $\mu_\xi(B) = \P\{\xi \in B\} = \P\big\{\xi^{-1}(b)\big\}$. Для того, чтобы понять как считать интеграл по распределению вероятности, введём понятие плотности. На прямой есть замечательная мера $dx$, которая является мерой длины, для объединения отрезков это знали ещё в античную эпоху. Она продолжается на сложные множества. Давайте попробуем выписать меру $\mu$ выраженную через $dx$.

\begin{Def}
  Пусть $\mu(B) = \int\limits_B \rho_\xi(x)\,dx = \P\{\xi \in B\}$. Тогда $\rho_\xi(x)$ называется плотностью распределения вероятностей.
\end{Def}

Вероятность того, что $\xi$ попала в $B$, можно переписать как интеграл по всему объемлющему множеству, используя индикатор $J_B(x)$.
\[
  \mu_\xi(B) = \P\{\xi \in B\} = \int\limits_{-\infty}^{+\infty} J_B(x) \rho_\xi(x)\,dx = 
  \int\limits_{-\infty}^{+\infty} J_B(x)\,\mu_\xi(dx).
\]
Мы видим, что если в качестве функции $f$ подставить индикатор $J_B(x)$, то интеграл по $\rho(x) dx$ и интеграл по мере $\mu_\xi(dx)$ совпадают. Таким образом мы можем и для простых функций (линейных комбинаций индикаторов)
\[
  \int\limits_{-\infty}^{+\infty} f(x) \rho_\xi(x)\,dx = \int\limits_{-\infty}^{+\infty} f(x)\,\mu_\xi(dx).
\]
Раз для простых, то и для измеримых существование плотности понятно. Совсем плохих функций в приложениях не бывает.

Всё изложено в предположении, что все интегралы и суммы сходятся в смысле абсолютной сходимости. Лебеговские интегралы считаются существующими, если сходятся в абсолютном значении.

Возьмём частный случай $f(x) = x$.
\[
  \E f(\xi) = \E\xi = \int\limits_\R x \rho_\xi(x)\,dx.
\]
Очень понятная формула. Вопрос: а почему же надо таскать с собой все эти $\sigma$-алгебры, почему нельзя определить мат. ожидание сразу через плотность? Так поступить математик абсолютно не может вот по какой причине. Давайте случайную величину $f(\xi)$ обозначим через $\eta$. У случайной величины тоже есть плотность $\rho_\eta(x)$. Тогда
\[
  \E f(\xi) = \int\limits_{\R} f(x)\,\mu_\xi(dx) = \int\limits_R f(x) \rho_\xi(x)\,dx.
\]
С другой стороны
\[
  \E\eta = \int\limits_\R x \rho_\eta(x)\,dx.
\]

Нам надо доказать такое правило обращения с интегралами
\[
 \int\limits_\R f(x)\, \rho_\xi(x)\,dx = \int\limits_\R x\,\rho_{f(\xi)}\,dx.
\]
Нет такого правила в математическом анализе, но оно верно. Для доказательство этого правила нам и нужен формализм Колмогорова.

Чего нет в математических учебниках: плостность распределения "--- величина размерная. Математики обычно считают всё по умолчанию безразмерным, на практике вы можете действовать и не так. Однако всегда можно от размерности освободиться в самом начале.

Пусть $\xi = (\xi_1,\dots,\xi_n) \in \R^n$. А функция $f(x)\in \R^1$, но $x = (x_1,\dots,x_n)$. Изменится ли что-нибудь в нашей формуле?
\[
  \E f(\xi) = \int\limits_{\R^n} f(x)\,\mu_\xi(dx) = \int\limits_{\R^1} f(x)\,\rho_\xi(x)\,dx.
\]
Плотность теперь надо понимать не по длине, а по объёму.

Пусть ещё $\xi_1^2 +\xi_2^2+\dots + \xi_n^2 =1$, то есть $(\xi_1,\dots,\xi_n)\in S^{n-1}$. Но ведь на сфере есть замечательная мера. И снова ничего не меняется в наших формулах. Это большое преимущество.

Если $\rho_\xi(x)$ кусочно-гладкая, имеем
\[
  \rho_\xi(x) = \lim\limits_{O(x)\to \{x\}} \frac{ \P\big\{\xi \in O(x)\big\}}{V\big(O(x)\big)}.
\]

Математические науки как устроены: есть несколько основных формул. Из них выводится множество других более узких. В теории вероятностей таких основных формул всего две.

Пусть имеются две области $X$ и $Y$ в $\R^n$. Пусть имеется гладкое взаимнооднозначное отображение $y = f(x)$, $f = (f_1,f_2,\dots, f_n)$. Пусть случайная величина $\xi$ принимается значения в $X$. Можно с помощью $f$ определить случайную величину $\eta = f(\xi)$, $\xi = (\xi_1,\dots,\xi_n)$, $\eta = (\eta_1,\dots, \eta_n)$. Наша задача состоит в том, чтобы выразить плотность $\rho_\eta(y)$ случайной величины $\eta$ через плотность случайной величины $\rho_\xi(x)$.

Сейчас мы будем её искать.
\begin{multline*}
  \rho_\eta(y) = \lim\limits_{O(y)\to \{y\}} \frac{\P\{\eta\in O(y)\}}{V\{O(y)\}} = 
   \lim\limits_{O(y)\to \{y\}} \frac{\P\{ f(\xi) \in O(y)\}}{V\{O(y)\}} = \\
 = \lim\limits_{O(y)\to \{y\}} \frac{\P\big\{ \xi\in f^{-1}\big[O(y)\big]\big\}}{V\{O(y)\}} =\\
 \cmt{сделаем деликатную вещь: прообраз $O(y)$ есть некая другая окрестность $O\big(f^{-1}(y)\big)$}\\
 = \lim\limits_{O(y)\to \{y\}} \frac{\P\big\{ \xi\in O [f^{-1}(y)]\big\}}{V\{O[f^{-1}(y)])\}} \frac{ V\big\{ O[f^{-1}(y)] \big\}}{ V\big( O(y)\big)} =\\
 = \rho_\xi\big( f^{-1}(y)\big) \big|(\J f^{-1})(y)\big|
\end{multline*}

Можем ли мы хоть для чего-нибудь использовать эти все знания. Зачем мы всем этим занимаемся. Все эти плотности, верояности, мат ожидания, не могут быть совсем уж произвольными вещами. Есть некий таинственный список распределений, которыми пользуются. Он меняется от эпохи к эпохе.

\subsection{Ошибка округления}
Есть шкала, расстояние между делениями $\Delta$, есть стрелка, которая останавливается между делениями шкалы. Ошибка может меняться в интервале $\left[ -\frac\Delta2,\Delta2 \right]$. А можем ли мы догадаться, какое распредление вероятности этой ошибки, какая плотность? Вроде бы всё равно, куда попасть, значит, постоянная плотность должна быть внутри отрезка, а вне отрезка равна нулю.
\[
  \rho(x) = \begin{cases}
  0,&x\not\in \left[ -\frac\Delta2,\frac \Delta2 \right];\\
  c,& x\in\left[ -\frac\Delta2,\frac \Delta2 \right].
\end{cases}
\]
А как найти $c$. Надо, чтобы вероятность всей прямой была единицей, значит, $c = \frac1\Delta$. Таким образом, $\rho_\xi(x) = \frac\Delta J\left\{ -\frac\Delta2\le x\le \frac\Delta 2 \right\}$.

Определим ещё одну случайную величину $\eta = a + b\xi$. Тогда образ отрезка при линейном отображении "--- тоже отрезок. Получим аналогичное «равномерное» распределение, но уже на некотором отрезке $[c,d]$. 

Чему равно математическое ожидание $\xi$? $\E\xi = \int\limits_{-\Delta/2}^{\Delta/2} x\frac1\Delta \,dx = 0$. Умея вычислять мат ожидание, мы фактически обладаем понятием дисперсии
\[
  \D\xi = \E (\xi - \E\xi)^2 = \int\limits_{-\infty}^{+\infty} (x-\E\xi)^2 \rho_\xi(x)\,dx = \frac{\Delta}{12}.
\]
Аналогично, $\D\eta = \frac{(d-c)^2}{12}$.

Раз знаем дисперсю, хотим перетащить на эту теорию закон больших чисел. Для этого надо ввести понятие независимых случайных величин. Это позже. Давайте ещё пример посмотрим.
\subsection{Момент отказа}
Пусть компьютер работает при $t=0$ и есть момент $\tau>0$, в который он откажет. Введём случайую величину $Q_\tau(t) = \P\{\tau > t\}$. Рассмотрим приращение
\[
  Q_\tau(t + \Delta t) = Q_\tau (t) \big( 1 - \lambda(t)\Delta t + o(\Delta t)\big).
\]
Составляем дифференциальное уравнение
\[
  \Delta Q_\tau(t) = -\lambda(t)  Q_\tau(t) \Delta t + o(\Delta t);\qquad
  \frac{ }{dt}Q_\tau(t) = -\lambda(t) Q_\tau(t).
\]
Отсюда получается, что
\[
  Q_\tau(t) = e^{-\int\limits_0^t \lambda(s)\,ds}.
\]
Пусть $\lambda(t) = \const$, то есть от $t$ никак не зависит. Тогда $Q_\tau(t) = e^{-\lambda t}$. Хотим поняти, какова плотность этого распределения
\[
  e^{-\lambda t} = \int\limits_t^{+\infty} \rho_\tau (s)\,ds.
\]
Таким образом,
\[
  \rho_\tau(t) = \begin{cases}
  \lambda e^{-\lambda t},& t\ge 0;\\
   0 & t<0.
\end{cases}
\]
Такое распределение называется показательным.

Несложными вычислениями можно показать, что $\E\tau = \frac1\lambda$, $\D\tau = \frac1{\lambda^2}$.

\subsection{Нормально распределение}
Оно используется для оценки ошибок общего характера. Разберём случай, обозначаемый $\xi \sim N(0,1)$. Определим плотность
\[
  \rho_\xi(x) = \frac1{\sqrt{2\pi}} e^{-\frac{x^2}2}.
\]
С помощью этого распределения мы уточним неравенство Чебышёва, но чуть позже.

Общее нормальное распределение получается из стандартного $N(0,1)$ линейной заменой
\[
  \eta = \sigma \xi + a,\quad
  f(x) = \sigma x + a = y;\quad
  f^{-1}(y) = \frac{y - a}{\sigma} = x.
\]
Тогда
\[
  \rho_\eta(y) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(y-a)^2}{2\sigma^2}}.
\]
Вычисления показывают, что $\E\xi = 0$, $\E\eta = a$, $\D\xi = 1$, $\D\eta = \sigma^2$. Пишут $\eta\sim N(a,\sigma^2)$.

Мечта ещё со времён Лапласа, чтобы все ошибки описывались нормальным законом с некоторыми $a$ и $\sigma^2$. Но не обязательно вообще ощибка описывается с помощью распределения.
