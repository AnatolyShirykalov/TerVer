\section{Лекция 10}
Пусть $\xi\sim N(0,1)$. То есть $\E\xi=0$, $\sqrt{\D\xi} = 1$. Пусть также $\eta = \sigma\xi + a$, где $\sigma\ne0$. Тогда $\E\eta = a$, $\D\eta = \sigma^2$.

Пусть мы хотим вычислить
\[
  \underbrace{\int\limits_0^1\ldots\int\limits_0^1}_n f(x_1,\dots,x_n)\,dx_1\,\dots\,dx_n = J.
\]

Можем применить вероятностный подход.
Зададим последовательность случайных векторов $\xi^{(i)} = \left( \xi_1^{(i)},\xi_2^{(i)},\ldots,\xi_n^{(i)} \right)$. Тогда
\[
  \E f(\xi^{(1)}) = \int\limits_0^1\dots\int\limits_0^1 f(x_1,x_2,\dots,x_n) \cancelto1{p_{\xi^{(1)}}(x_1,\dots,x_n)}\,dx_1\dots\,dx_n = J.
\]
При этом $s^2 = \frac1{n-1} \sum (x_i-\ol x)^2$. И величина $\frac{\left( \ol{f(\xi)} - J \right)}{\sqrt n}\sim N(0,1)$. Это будет чем-то похожим на нормальное распределение.

Обозначим номер последнего наблюдения за $N$. Какое надо взять число наблюдений, чтобы получить хорошую точность?

Пусть есть два набора наблюдений $x_1,\dots,x_m$, $y_1,\dots,y_n$. Насколько выводы по этим наблюдениям согласованы? Как известно, надо составить эмпирическую функцию распределения.
\[
  \sup\limits_x \big|F_m(x) - F_n(x)\big|\sqrt{\frac{mn}{m+n}}
\]
будет статистикой, называемой функцией Колмогорова. Перебрать все выборки невозможно. Даже пять или десять выборок сложно обработать. С появлением Компьютера любое распределение можно приблежённо расчитать с помощью метода Монте-Карло.

Займёмся примерами из медицины. Дело сведётся к испытаниям Бернулли. У нас будет $n$ испытаний, $p$ вероятность успеха в одном испытании, $\mu$ "--- общее число успехов. Тогда $\E\mu=np$, $D\mu = npq$, где $q=1-p$. Займёмся клиническими испытаниями по следующей схеме. Больные будут появляеться не по одиночке, а парами. Есть два метода лечения $A$ и $B$. Методом бросания монетки решается, какой из больных получит лечение методом $A$. И хотим выяснить, какой из методов лучше. Если предполагаем, что методы эквивалентны, то $p=\frac12$ "--- вероятность того, что на одной паре $B$ окажется лучше $A$. Пусть $\mu$ "--- число побед $B$ в $n$ испытаниях. Пусть $n=100$, $E\mu = 50$, $\sqrt{\D\mu} = 5$. В реальности $\mu$ может отклониться от мат ожидания на один корень из дисперсии легко, на два корня с трудом, на три едва ли. Таким бразом $\E\mu + 2 \sqrt{\D\mu} = 60$. $\P\{\mu\ge60\} \approx 0.02$. Это уровень значимости.

Пусть теперь $p=0.6$ и $\E\mu=60$. Тогда вероятность $\frac12$ того, что $\mu\ge 60$. Это расчёт мощности.

\subsection{Метод наименьших квадратов}
Наши наблюдения имеют вид $x_i = a_i + \delta_i$, причём $\delta_i \sim N(0,\sigma)$. При этом $\sigma$ иногда известна, а иногда оценивается. $\delta_i$ "--- НОРСВ соответственно. Вектор $a = (a_1,\dots,a_n) \in L$ "--- некоторое линейное подпространство $\R^n$. Можно даже предположить, что $a\in M = L +b$, где $L$ и $b$ известны. Но вычитая, $b$ из всех $a_i$, сведём этот случай к $M=L$. При этом $\D\delta_i = \frac{\sigma^2}{w_i}$, где $w_i$ "--- вес, то есть число измерений, в результате которых получилось число $x_i$. Рассмотрим $y_i = x_i \sqrt{w_i}$, $b_i = a_i\sqrt{w_i}$. Таким образом задача сводится к равным весам.

Пример. Измеряем углы $a_1,a_2,a_3$ треугольника. Тогда $a_1+a_2+a_3 = \pi$. При этом наши измерения $x_i = a_i + \delta_i$ уже этим свойством не обладают. Рассмотрим $y_i = a_i - a_i^0 + \delta_i$, где $a_i^0$ "--- какая-то грубая оценка углов треугольника, но дающая $a_1^0 + a_2^0 + a_3^0 = \pi$. Обозначим $b_i = a_i - a_i^0$. Тогда $\Sigma b_i = 0$. Свели к линейному подространству.

Или пусть мы измеряем грузы $a_1,a_2$. При этом наблюдения $x_i = a_i + \delta_i$ для $i=1,2$. Но ещё делаем наблюдение $x_3 = a_1 + a_2 + \delta_3$. Тут тоже $x_3\ne x_1+_2$.

В общем. Есть величины $a_1,\dots, a_n$. Если какие-то уравнения $F_1(a_1,\dots, a_n) = 0,\F_2(a_1,\dots,a_n) = 0, \dots$. Вводим $a_1 = a_1^0 + \Delta a_1,\dots a_n = a_n^0 + \Delta a_n$. Неизвестными являются только поправки $\Delta a_n$. В точке $(a_1^0,\dots, a_n^0)$ пишем линейные приближения наших уравнений $F$.

Для $n=3$ и $\dim L = 2$ получается такая картинка. Где-то в плоскости лежит $a$. Но дьявол вносит ошибки в мои наблюдения и $x$ в $L$ не лежит. Поэтому можно спроецировать $x$ на $L$. $\hat a = \arg \min\limits_{a\in L}\|x-a\|^2$.

То, что мы предположитли о наблюдениях, как реализациях случайных величин, запишем в следующем виде
\[
  p_i(x_i) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left\{ -\frac{(x_i-a_i)^2}{2\sigma^2} \right\},\quad
  p_1(x_1)\dots p_n(x_n) = \left( \frac{2}{\sigma\sqrt{2\pi}} \right)^n\exp\left\{ -\frac{1}{2\sigma^2}\sum(x_i-a_i)^2 \right\}\to \max\limits_{a\in L}
\]
Нахождение такого максимума (эта функция называется функцией правдоподобия) равносильно минимизации $\sum(x_i-a_i)^2\to\min\limits_{a\in L}$.

Вопрос остаётся, насколько велико отличие $\hat a - a$. Запишем, что $x = a+ \delta$. Тогда $\hat a = a + \mathrm{proj}_L \delta$. Таким образом $\hat a - a = \mathrm{proj}_L\delta$. Плотность $\delta_i$ имеет вид
\[
  p_{\delta_i}(x_i) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{x_i^2}{2\sigma^2}}.
\]
Значит, совместная плотность есть произведение этих плотностей
\[
  p_\delta(x) = \left( \frac{1}{\sigma\sqrt{2\pi}} \right)^n\exp\left\{ -\frac{1}{2\sigma^2}\sum x_i^2 \right\}
\]
При ортогональном преобразовании сумма квадратов компонент вектора не меняется. Пусть $u$ "--- ортогональное преобразование. Выберем его так, чтоб $uL = \la e_1,\dots, e_k\ra\ni \delta$. Тогда $\proj_{L}\delta = (\delta_1,\delta_2,\dots,\delta_k,0,\dots,0)$.  Если бы мы знали $\delta_i$, то оценили бы $\sigma^2 = \E \sigma_i^2$. Нас интересует ортогональная составляющая
\[
  \proj_{L^\perp} x = \proj_{L^\perp} \delta = (0,0,\dots,0,\delta_{k+1},\dots, \delta_n).
\]
При этом, учитывая $\delta_i =\sigma\xi_i$, где $\xi_i\sim N(0,1)$, имеем  $\|\proj_L x\|^2 = \RY j{k+1}n \delta_j^2 = \sigma^2\RY j{k+1}n\xi_j^2$.

Нам надо оценить величину $\sigma$. Пусть $x$ "--- наблюдения сейчас. Проекция $\proj_L x = \hat a$, мы её умеем вычислять, особенно если у нас компьютер. А ортогональная составляющая $\proj_{L^\perp} x = x - \hat a = \Delta$ "--- вектор кажущихся ошибок.
Размерность обозначим $\dim L = k$. Тогда $\dim L^\perp = n-k$.
\[
  \frac1{n-k}\|\delta\|^2 = \sigma^2 \frac1{n-k}\RY j{k+1}n\xi_j^2.
\]
Число $n-k=f$ называется числом степеней свободы. Этот термин из теории распределения газа. Чем больше степеней свободы, тем больше
\[
  \frac1{n-k} \RY j{k+1}n\xi_j^2
\]
похож на единицу.

В девятнадцатом веке всё считали на бумаге. И вводили следующее распределение. Пусть $\xi_i\sim N(0,1)$. Тогда $\RY i1m \xi_i^2 = \chi_m^2$ "--- распределение хи-квадрат (К.",Пирсон).
Для функции распределения этой величины составляли таблицы. Через эти таблицы можно оценить $\sigma^2$. Как раз обозначают оценку, как обычно
\[
  s^2 = \frac1{n-k} \|\Delta\|^2.
\]

Пусть есть выборка $x_1,\dots,x_n$. Пусть можно записать $x_i = a+\delta_i$, то есть $a_i = a$ все одинаковые. Давайте спроектируем. У нас есть $L = \la (1,\dots,1)\ra$ "--- одномерное подпространство. Спроецируем
\[
  \frac{\big((x_1,\dots,x_n,(1,\dots,1)\big)}{n}(1,\dots,1) = (\ol x,\dots,\ol x).
\]
Считаем $\Delta = (x_1 - \ol x,\dots,x_n-\ol x)$, $s^2 = \frac1{n-1}\sum\limits_i(x_i-\ol x)^2$. Вот мы новым методом получили ту же оценку, к которой уже привыкли.
