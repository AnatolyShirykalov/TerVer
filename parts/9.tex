\section{После цпт}
Эта теорема состояла в следующем. Если есть независимые одинаково распределённые случайные величины $\xi_1,\dots,\xi_n,\dots$ с характеристиками $\E\xi_k = a$, $\D \xi_k = \sigma^2$, можно ввеси $S_n = \xi_1+\dots+\xi_n$ и $s_n = \frac{ S_n - na}{\sigma\sqrt{n}}$. Так вот $s_n$ слабо сходится к нормальному распределению, то есть к распределению $\nu$, имеющему плотность
\[
  p_\nu(x) = \frac1{\sqrt{2\pi}}e^{-\frac{x^2}2}.
\]
При этом $\P\big\{|\nu|\le 1\big\} = 0.84$, $\P\big\{|\nu|\le 1.96\big\} = 0.95$, $\P\big\{|\nu|\le 2.57\big\} = 0.99$, $\P\big\{|\nu|>3\big\} = 0.003$.

Ну а тогда ведь $S_n = na + \sigma\sqrt{n} s_n$. Конечно, $\E S_n = na$ и $|na|\gg \sigma\sqrt n$ при $a\ne 0$.

Значит, если вам предлагается сыграть в азартную игру. Делаете ставки $\xi_1,\dots, \xi_n,\dots$, вам первым делом стоит поинтересоваться знаком $a$. Если $a>0$, стоит играть, но так никогда не бывает. А если $a<0$, то надо уходить. Выйграть можно только играя в не совсем случайную игру.

Что мы имеем, когда хотим написать так называемый доверительный интервал. Имеем данные $x_1,\dots, x_n$. В высшей степени тёмное дело объяснить, почему мы считаем, что эти иксы есть реализация независимых одинаково распределённых случайных величин. Но за неимением лучшего мы именно так и считаем. Тогда $x_n = a + \delta_i$. В высших сферах есть генератор случайных чисел и один ансабль реализаций генератора мы имеем в выборке. $n$ называется объёмом выборки (оно же число наблюдений). Выводы не обладают абсолютной надёжностью. Но вот принято путать обозначения в учебниках. С одной стороны $x_1,x_2,\dots,x_n$ "--- результаты измерений, а с другой стороны обознают теми же буквами случайные величины, которые якобы лежат за тем, что мы имеем. Пишут
\[
  \D x_i = \D \delta_i = \sigma^2.
\]
Поскольку все одинаковые, можно обозначить за $\sigma^2$.

Ну вот не хватает букв в математике, обозначают разные вещи одной буквой. Иногда всё-таки случайные величины обозначают $X_1,\dots, X_n$.

Как много раз я объяснял, оценкой дисперсии является $\sigma^2 \approx s^2 = \frac1{n-1} \RY i1n(x_i-\ol x)^2$.

Давайте поймём, откуда взялась минус единица. Пусть $S^2 = \frac1n \RY i1n(x_i-\ol x)^2$. Тут сейчас снова возникнет путаница в обозначениях реализации и случайной величины. Будем считать мат ожидание, уже подставив вместо наблюдений случайные величины. Можно ещё соорудить случайную величину
\[\eta\sim
\begin{pmatrix}
 x_1 & x_2 & \dots x_n\\
 \frac1n & \frac 1n & \dots & \frac1n
\end{pmatrix}.
\]
Тогда $\E S^2 = \D \eta$. Ну давайте считать, подставив $x_i$ как случайные величины. Давайте для простоту считать, что $a=0$, всё равно они сокращаются.
\begin{multline*}
  \E S^2 = \E\frac1n \RY i1n (x_i-\ol x)^2 = \E \bigg(\frac1n\RY i1n x_i^2 - \bigg[\frac1n\RY i1n x_i\bigg]^2\bigg) = \frac1n (n\sigma^2) - \frac1{n^2} \E \sum\limits_{i,j=1}^n x_ix_j = \\
  = \sigma^2 - \frac1{n^2} \E \RY i1n x_i^2 = \sigma^2 - \frac{\sigma^2}n = \bigg(1-\frac1n\bigg)\sigma^2.
\end{multline*}

А нам бы хотелось иметь мат ожидание оценки $\sigma^2$. Поэтому считаем $\E s^2 = \sigma^2$. Это пример того, как изучаются характеристики наблюдений, возводя наблюдения в ранг случайных величин. Если наблюдений много, хотя бы несколько десятков, разница $S^2$ и $s^2$ ничтожна:
\[
  S^2 = \underbrace{\frac 1n\sum x_i^2}_{\to\E x_i^2} - \underbrace{(\ol x)^2}{\to (\E x_i)^2}.
\]
Сходимостм слабые.

Обозначают $\ol x = \hat a$. Имеем $\D \ol x = \frac{\sigma^2}n$. Соорудим центрированную и нормаированную величину. Она тогда близка к стандартному нормальному, а значит
\[
  \P \bigg\{\bigg|\frac{\ol x - a}{\frac{\sigma}{\sqrt{n}}}\bigg| \le 1.96\bigg\} = 0.95.
\]
При этом мы подразумеваем, что оценили дисперсию $\sigma\approx s$. Тогда
\[
  \P\bigg\{|\ol x - a|\frac{\sqrt n}s \le 1.96\bigg\} \approx 0.95.
\]
Чуть-чуть огрубим
\[
  |\ol x - a| \le 2\frac s{\sqrt{n}}.
\]
Может быть наша выборка оказалась среди тех пяти процентов, которые плохие. $0.95$ называется доверительной вероятностью. Доверительный интервал определяется последним интервалом.

Из неравенства Чебышёво получалась оценка более грубая.

\subsection{Эмпирическая функция распределения}
Почему мы вообще наделяем наблюдения сущностью случайных величин.  Есть один приём.

Если у вас есть много чисел, хотя бы штук 50. Вас будет тошнить, если вы на них посмотрите. Но существует приём, с помощью которого вы можете эту тошноту преодолеть.

Пусть $x_1,\dots, x_n$ "--- опытные данные. Для случайной величины
\[
  \begin{pmatrix}
 x_1 & x_2 & \dots x_n\\
 \frac1n & \frac 1n & \dots & \frac1n
\end{pmatrix}.
\]
строится функция распределения $\mathcal F_n(x) = \frac{\text{число }x_i<x}m = \text{доля}(x_i<x)$. Для $-\infty<x<\infty$. Возникает понятие вариационного ряда
\[
  x_{(1)}\le x_{(2)}\le \dots \le x_{(n)}.
\]

Левее точки $x_{(1)}$ функция ноль. Когда достиг, тоже ноль. А как только сдвинулись чуть правее $x_{(1)}$ получается ступенька. На последнем наблюдении $x_{(n)}$ выйдем на единичку.

Если вы такой график нарисуете, то чувство тошноты у вас уже пройдёт. В этом графике вы должны наблюдать что-то гладкое и хорошее. Теоретическое приближение. Хочется наблюдать, конечно, нормально распределени.

Мы обозначали $\nu\sim N(0,1)$. А общий вид $\mu = a + \sigma\nu$  для $\sigma>0$. Вводится функция Лапласа, которая не выражается в элементарных функциях
\[
  \Phi(x) = \P\{\mu < x\} = \P\bigg\{\nu < \frac{x-a}\sigma\bigg\} = \Phi\left( \frac{x-a}\sigma \right).
\]
Соответственно $\Phi^{-1}\big(\P\{\mu < x\}\big) = \frac{x-a}\sigma$.

В Excel проще работать столбиками. Стоблец с $x_i$ сортируете, получаете столбец с $x_{(i)}$. Дальше
\[
  \begin{matrix}
 x_1 & x_{(1)} & \frac1{2n} & \Phi^{-1}\left( \frac1{2n} \right)\\
 x_2 & x_{(2)} & \frac1{2n} + \frac 1n & \Phi^{-1}\left( \frac1{2n} + \frac1n \right)\\
 \vdots & \vdots &\vdots & \vdots \\
 x_n & x_{(n)} & 1 - \frac1{2n} & \Phi^{-1}\left( 1 - \frac1{2n}\right)
\end{matrix}
\]
Как понять, похожа эмпирическая функция на прямую или не похожа. Колмогоров предложил $\sup \big| F_n(x) - F(x)\big|$. Здесь $F(x) = \P\{\xi < x\}$ "--- теоретическая функция распределения. Теперь делаем монотонное преобразование $\eta = g(\xi)$, то есть $g$ монотонно возрастает. Так же преобразуем $y_i = g(x_i)$, То есть будем наблюдать вместо $x$ значение $g(x)$. Расстояния по вертикали между значениями функций распределения не произойдёт. Если у меня есть две монотонных непрерывных, то я могу подобрать монотонную, которая переводит одну в другую. Будем брать функцию распределение статистики Колмогорова
\[
  \P\Big\{\sqrt n \big|F_n(x) - F(x)\big| < y\Big\} \te K(y).
\]
Функция $K(y)$ называется функцией Колмогорова. Сходмость здесь достаточно быстрая, с нескольких десятков наблюдений уже можно пользоваться.
\[
  \begin{matrix}
\text{уровни}&  0.15 & 0.10 & 0.05 & 0.02 & 0.01\\
\text{значения}&  1.14 & 1.23 & 1.36 & 1.52 & 1.63
\end{matrix}
\]
То есть $1 - K(1.14) = 0.15$, $1-K(1.36) = 0.05$.

В результате из функции $F_n(x)$ вычитаем $\Phi\left( \frac{x-\ol x}s \right)$. Значения этой статистики как-нибудь обозначим
\[
  \sup\limits_x \bigg| F_n(x) - \Phi\left( \frac{x - \ol x}{s} \right)\bigg| = \hat D_n.
\]
Если взять для оценки нормальности распределения вот такую статистику (умножим её на зависящий от $n$ коэффициент)
\[
  \hat D_n(\sqrt{n} - 0.01 + 0.85/\sqrt n)\quad
  \begin{matrix}
\text{уровни}&  0.15 & 0.10 & 0.05 & 0.02 & 0.01\\
\text{значения}& 0.775 & 0.819 & 0.895 & 0.955 & 1.035
\end{matrix}
\]
Если параметры распределения $a,\sigma$ заранее известны, то нормальность оцениваем Функцией Колмогорова. Если оцениваем по выборке, используем вторую функцию. Это называется критерием Лиллифорса.

На века приёмы построения доверительных интервалов не годятся. А вот когда вы измеряете одно наблюдение в понедельник, второне во вторник, есть шансы на адекватность.

Пусть есть две серии наблюдений $x_1,\dots x_m$, $y_1,\dots,y_n$. Вы хотите, чтобы результаты согласовывались. Мы вычисляем $\ol x$, $s_x$, $\ol y$, $s_y$. Рассмотрим разницу $\ol x - \ol y$. Хотим, чтобы $\E(\ol x - \ol y)=0$, $\D(\ol x - \ol y) = \D\ol x + \D\ol y = \frac{s^2_x}m + \frac{s^2_y}n$. Мы считаем, что
\[
  \frac{\ol x-\ol y}{\sqrt{\frac{s_x^2}{m} + \frac{s^2_y}{n}}} \sim N(0,1).
\]

Можно с помощью эмпирических функций взять супремум, когда $m$ и $n$ велики (хотя бы по несколько десятков)
\[
  \sup\big| F_m(x) - F_n(x)\big| \sqrt{\frac{mn}{m+n}}
\]
Для больших $n,m$ эта штука имеет распределение Колмогорова. Мы здесь работаем в предположении, что наши статистические функции равны.
