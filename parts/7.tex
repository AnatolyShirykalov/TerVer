\section{Лекция 7}
В чём состоят основы теории вероятности. Постараюсь в одном месте все их написать. Как и полагается в аксиоматике Колмогорова множество элементарных событий $\Omega$, на нём есть $\sigma$-алгебра $\mathfrak B$, и есть вероятность (мера) $\P$. Кроме того есть функции $\xi =\xi(\omega)\in\R^1$, но не абы какие, но такие, что если $B\subset \R^1$ (борелевское), то $\xi^{-1} = \big\{ \omega\colon \xi(\omega) \in B\big\}\in\mathfrak B$. При этом
\[
  \P\big\{\xi^{-1}(B)\big\} = \P\{\xi\in B\} = \mu_\xi(B)
\]
называется вероятностной мерой или распределением $\xi$.

Можно сказать, что Колмогоров сказал больше, чем нужно в приложениях. Но если нам задана только вероятностная мера $\mu$ на прямой, то ничего не стоит создать и $\Omega$, и $\mathfrak B$, $\P$ и $\xi$, для которых $\mu = \mu_\xi$. Возьмём $\Omega = \R$, $\mathfrak B = \{$борелевские$\}$, $\P = \mu$, $\xi(\omega) = \omega$. Таким образом, Колмогоров не запросил ничего лишнего.

Дальше, что такое математическое ожидание.
\[
  \E\xi = \int\limits_\Omega \xi(\omega)\P(d\omega).
\]
Оказывается, что если есть какая-то функция $f(x)$, то функция $\eta = f(\xi) = f\big(\xi(\omega)\big)$ тоже будет являться случайной величиной и можно вычислить мат ожидание
\[
  \E f(\xi) = \int\limits_\R^1 f(x)\,\mu_\xi(dx).
\]
При это доказать это без $\Omega$ и $\omega$ не получится. Доказывают равенства интегральных сумм для интеграла
\[
  \E f(\xi) = \int\limits_\Omega f\big(\xi(\omega)\big)\P(d\omega).
\]
Однако такие формулы невозможно использовать для расчётов. Интегральные суммы считать "--- дело неблагодарное. Но вводится функция плотности.

Пусть существует такая функция от $x$, что
\[
  \int\limits_B p_\xi(x)\,dx = \mu_\xi(B).
\]
Ну и легко видеть, что наконец-то для случая, когда существует плотность
\[
  \E\big(f(\xi)\big) = \int\limits_\R f(x)\,\rho_\xi(x)\,dx.
\]
А это уже то, что вполне возможно как-то посчитать.

Далее если мы знаем, что такое мат ожидание, то и знаем, что такое дисперсия.
\[
  \D\xi = \E (\xi - \E\xi)^2.
\]
Любимое обозначения для математического ожидания $a= \E\xi$. Тогда
\[
  \D\xi = \E (\xi-a)^2 = \int\limits_\R (x-a)^2p_\xi(x)\,dx.
\]
А это мы умеем вычислять.

Но если не существует плотности, тогда в самом общем случае
\[
  \D\xi = \int\limits_\R (x-a)^2\mu_\xi(dx).
\]
Глядя на эти равенства, мы видим, что неравенство Чебышёва тоже будет иметь место. В чём оно заключается
\begin{Ut}
  $\P\big\{|\xi - \E\xi| \ge \e\big\} \le \frac{\D\xi}{\e^2}$.
\end{Ut}
\begin{Proof}
Доказывается это вот как
\[
  \D\xi\ge \int\limits_{|x-a|\ge \e} (x-a)^2\,\mu_\xi(dx) \ge \e^2\int\limits_{|x-a|\ge \e} \mu_\xi(dx) = \e^2\P\big\{|\xi-a|\ge \e\big\}.
\]
\end{Proof}
Ну раз у нас есть неравенство Чебышёва, то где-то недалеко закон больших чисел. Но для этого нам нужно ввести понятие независимых случайных величин.

\subsection{Закон больших чисел}
Возьмём две случайных величины $\xi,\eta$. Если для любых двух борелевских множеств $A,B\subset \R$ события
\[
  \big\{\omega\colon \xi(\omega)\in A\big\} = \{\xi\in A\}, \{\eta\in B\}
\]  
независимы, то есть
\[
  \P\{\xi\in A,\xi\in B\} = \P\{\xi\in A\}\P\{\eta\in B\}.
\]
Для конъюнкции двух событий используют знак умножения, пересечения или запятую. $\P\{\xi\in A,\xi\in B\} = \P\big\{(\xi,\eta)\in A\times B\big\}$. Здесь возникает мера на плоскости
\[
  \P\big\{(\xi,\eta)\in A\times B\big\} = \mu_{\xi\eta}(A\times B) = \mu_\xi(A)\mu_\eta(B).
\]
Здесь $\mu_{\xi\eta} = \mu_\xi\times\mu_\eta$.

Для чего я это рассказываю. У нас была теорема о математическом ожидании произведения случайных величин.

Пусть есть две независимые случайной величины $\xi,\eta$, пусть $\E|\xi|<\infty$, $E|\eta|<\infty$. Тогда
\[
  \E(\xi\eta) = \E\xi\eta = \E\xi\E\eta.
\]
Почему. Вот есть $\xi$. Когда строим интегральные суммы, мы заменяем $\xi$ на $\xi_n$ имеющую не более чем счётное число значений, для которой
\[
  |\xi- \xi_n|<\frac1n.
\]
И заметьте, что $\xi_n$ можно воспринимать, как функцию от $\xi$. Это нужно для того, чтобы говорить о независимости $\xi_n$ и $\eta_n$. Если случайные величины независимы, то и функции от этой пары случайных величин независимы. И наконец, во всех точках $\Omega$ есть сходимость $\xi_n\eta_n\to \xi\eta$ и можно пользоваться теоремой Лебега о предельном переходе под знаком интеграла.

Кто не помнит теорему Лебега, можно написать
\[
  \xi\eta - \xi_n\eta_n = \xi_\eta - \xi_n\eta + \xi_n\eta - \xi_n\eta_n.
\]
Расставляя скобки мы видим, что возникает оценка.

Отсюда получаем сразу, что для независимых $\xi,\eta$ и $\D(\xi+\eta) = \D\xi +\D\eta$. А отсюда мы сразу получаем закон больших чисел в том же виде.
И таким образом, всё что мы знали про дискретные случайные величины, мы перетащили на общих случай.
\subsection{Двигаемся дальше}
Пусть плотность случайной величины $\xi$ зависит от ещё каких-то параметров $p_\xi(x;\alpha,\beta,\gamma)$.

Кстати сказать, вместо $\R$ можно везде поставить $\R^n$, никакого отличия.

Нам нужно будет много случайных величин, бесконечное число. И в терминах плотности нужно получать плотности нескольких случайных величин.

Пусть $\xi,\eta$ независимы. И пусть $p_\xi(x),p_\eta(x)$ существуют.
Тогда на плоскости существует «совместная» плотность $p_{\xi,\eta}(x,y) = p_\xi(x)\cdot p_\eta(y)$. Надо это доказать
\begin{Proof}
Мы знаем, что $\mu_{\xi,\eta}(A\times B) = \mu_\xi(A)\cdot \mu_\eta(B)$. Когда существует плотность, имеем
\[
  \int\limits_A p_\xi(x)\,dx \int\limits_B p_\eta(y)\,dy = \iint\limits_{A\times B} p_\xi(x)p_\eta(y) \,dx\,dy.
\]
Но мера счётно-аддитивная однозначно определяется своими значениями на все прямоугольниках.
\end{Proof}

Не следует забывать, что кроме плотности есть ещё и функция распределения случайной величины. Возьмём функция $F_\xi(x):= \P\{\xi<x\}$ (иногда ставят $\xi\le x$). Конечно же
\[
  F_\xi(x) = \P\big\{\omega\colon \xi(\omega)<x\big\}.
\]
Если $x_1<x_2$, то $F_\xi(x_1)\le F_\xi(x_2$. Ну и если нужно посчитать вероятность попасть в полуинтервал
\[
  \P\{a\le \xi <b\} = \P\{\xi <b\} - \P\{\xi < a\} = F_\xi(b) - F_\xi(a).
\]
В случае существования плотности можно написать
\[
  \P\{a\le \xi<b\} = \int\limits_a^b p_\xi(x)\,dx = F_\xi(b) - F_\xi(a).
\]
Таким образом, $F_\xi$ является первообразно от $p_\xi(x)$.

\subsection{Распределение многомерной случайной величины}
Пусть $\xi = \{\xi_1,\xi_2\}$. По определение функцией распределения двумерной случайной величины называют
\[
  F_{\xi_1,\xi_2}(x_1,x_2) = \P\{\xi_1<x_1,\xi_2<x_2\}.
\]
Но на плоскости не одна система координат. Например, я могу оси повернуть. Как изменится функция распределения? Абсолютно непонятно.

Попытаемся снарядом попасть в некоего врага. Военные считают, что если при постоянном прицеле снаряды не будут попадать в одну и ту же точку. Есть некое рассеивание. Считают, что плотность вероятности имеет линии уровня в виде эллипсов. Двумерную функцию распределения не использовали в докомпьютерную эпоху. Сложно было считать.

Легко убедиться, что для независимых $\xi,\eta$ выполняется
\[
  F_{\xi,\eta}(x,y)F_\xi(x) F_\eta(y).
\]
Для других такого нет.

Пусть $\xi_1,\xi_2,\dots,\xi_n$ независимы. Тогда имеется совместная плотность $\rho(x_1,x_2,\dots,x_n)$. Можно взять $\eta = f(\xi_1,\xi_2,\dots,\xi_n)$. Как получить из многомерной плотности набор каких-то случайных величин. Ответ вот какой для двумерного случая.

Пусть $\xi,\eta$ случайные одномерные. Тогда
\[
  p_\xi(x) = \int\limits_\R p_{\xi,\eta}(x,y)\,dy.
\]
Понижение размерности производится путём интегрирования по всем остальным значениям.
\begin{Proof}
  Что это значит. Это значит, что вероятность
\[
  \P(\xi\in B) = \int\limits_Bp_\xi(x)\,dx.
\]
Мы докажем, что это так. $\P\{\xi\in B\} = \P\{\xi\in B,\eta\in \R\} = \int\limits_B \int\limits_\R p_{\xi\eta}(x,y)\,dy\,dx = \int\limits_Bp_\xi(x)\,dx$.
\end{Proof}

\subsection{Суммы случайных величин}
В основном рассматривают суммы независимых случайных величин. Мы хотим оценивать, насколько $\ol x = \frac1n\Sigma x_i$ отличается от истинного значения $a$. Поэтому случайные величины важны. И мы ими сейчас займёмся.

Пусть $\xi_1,\xi_2$ независимы и имеют плотности. Тогда
\[
  p_{\xi_1,\xi_2}(x_1,x_2) = p_{\xi_1}(x_1)p_{\xi_2}(x_2).
\]
Положим $\eta_1 = \xi_1+\xi_2$, $\eta_2 = \xi_2$ (чтобы было обратное преобразование). Тогда сделаем на плоскости замену $y_1 = x_1 + x_2$ и $y_2 = x_2$.

Надо вспомнить формулу. Если $y = f(x)$, где $x,y$ "--- векторы, но отображение гладкое и взаимнооднозначное: $x = f^{-1}(y)$. Пусть $\xi$ "--- случайный вектор с плотностью $p_\xi(x)$ (относительно неориентированного объёма). Положим $\eta = f^{-1}(\xi)$. Тогда
\[
  p_\eta(y) = p_\xi\big(f^{-1}(y)\big)\big| Jf^{-1}(y)\big|.
\]

В нашем случае обратное отображение $x_1 = y_1 - y_1$, $x_2 = y_2$. Понятно, что якобиан такого отображения есть единица.
\[
  p_{\eta_1,\eta_2}(y_1,y_2) = p_{\xi_1}(y_1-y_2) p_{\xi_2}(y_2).
\]
Если хотим только первое, то
\[
  p_{\eta_1}(y_1) = p_{\xi_1+\xi_2}(y_1) = \int\limits_{-\infty}^{+\infty} p_{\xi_1}(y_1-y_2)p_{\xi_2}(y_2)\,dy_2.
\]
Таким образом плотность суммы есть свёртка плотностей.

Лобачевский посчитал, как сложить $n$ случайных величин.

Есть такое чудо. Пусть есть независимые случайные величины $\xi_1,\dots,\xi_n$. Сумму обозначим через $S_n = \xi_1+\dots+\xi_n$. Чем больше слагаемых вы возьмёте, то больше будет область, где где значения не нулевые. Но раз она большая, то и плотность должна быть маленькая. Но Лаплас посоветовал взять нормированную сумму $s_n$. Вот, что это такое
\[
  s_n = \frac{ S_n - \E S_n}{\sqrt{\D S_n}}.
\]
Заметим, что $\E s_n = 0$ и $\D s_n = 1$. Суть состоит в том, что $s_n$ сходится к предельному нормальному закону с плотностью, которую я уже выписывал
\[
  \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left( -\frac{x^2}{2} \right).
\]
Для нормированной суммы с ростом $n$ будет более выделяться горб в нуле. Плотность уже не будет размазываться.

Самое удивительно, что можно даже увидеть такую сходимость для дискретных законов, например, если $\xi_i\in\{0,1\}$.

Пусть есть последовательность $\xi_1,\xi_2,\dots,\xi_n,\dots$ случайных величин и ещё случайная величина $\xi$.
\begin{Def}
Говорят, что $\xi_n\to \xi$ в слабом смысле, если
\[
  \forall\ f\in C_0^\infty\pau
  f(\xi_n) \to \E f(\xi).
\]
\end{Def}
Скоро мы увидим, что слабая сходимость случайных величин гарантирует сходимость вероятности попасть в заданный интервал. Именно в этом смысле будет доказывать сближение нормированных сумм случайных величин к нормальному закону.
