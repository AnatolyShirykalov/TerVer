\section{Лекция 4}
Имеем случайную величину $\xi =\xi(\omega)$, заданную законом распределения
\[
  \begin{pmatrix}
    a_1 & a_2 &\dots &a_n &\dots\\
    p_1 & p_2 &\dots &p_n &\dots
  \end{pmatrix}
\]
Здесь $p_i = \P\{\xi = a_i\} = \sum\limits_{\omega\colon \xi(\omega) = a_i}\P(\omega)$.

Ввели понятия математического ожидания
\[
 \E\xi = \sum\limits_{a_i} a_i p_i,
\]
дисперсии
\[
 \D\xi = \sum\limits_{a_i} (a_i - \E\xi)^2 p_i.
\]
Поскольку математическое ожидание определяется несколько иначе, а именно
\[
  \E\xi = \sum\limits_{\omega\in\Omega}\xi(\omega) \P(\omega),
\]
имеем аддитивность
\[
  \E(\xi+\eta) = \E\xi + \E\eta,\quad
  \D(\xi +\eta) = \D\xi + \D\eta + 2\E(\xi - \E\xi)(\eta-\E\eta).
\]
Для независимых случайных величин дисперсия аддитивна  и $\E(\xi\eta) = \E\xi \E\eta$.

Также обсудили неравенство Чебышёва
\[
  \P\big\{|\xi - \E\xi|\ge \e\big\} \le \frac{\D\xi}{\e^2}.
\]
Определим $\sigma = \sqrt{\D\xi}$ стандартное отклонение. Пусть $\e = 3\sigma$. Тогда
\[
  \P\big\{|\xi - \E\xi|\ge \e\big\} \le \frac{\D\xi}{\e^2} = \frac{\sigma^2}{9\sigma^2} = \frac19.
\]

Ещё доказали закон больших чисел, Пусть величины $\xi_1,\xi_2,\dots,\xi_n$ независимы и $D\xi_i<C$. Тогда
\[
  \D\left(\frac{\xi_1+\xi_2+\dots+\xi_n}{n}\right) = \frac1{n^2}\RY i1n \D\xi_i \le \frac{C n}{n^2} = \frac Cn\te 0.
\]

\begin{Def}
  Последовательность случайных величин $\eta_k$ сходится к нулю по вероятности, если
  \[
    \forall\ \e>0\pau \P\big\{|\eta_n|>\e\big\}\te 0.
  \]
  
  Также определяется $\eta_n\te\zeta$, если $\eta_n-\zeta\te 0$ по вероятности.
\end{Def}

Пусть $\xi_1,\xi_2,\dots,\xi_n,\dots$ независимые одинаково распределённые случайные величины, причём $\E\xi_n=a$. Тогда
\[
  \frac{\xi_1+\xi_2+\dots+\xi_n}n - a \tever 0.
\]
Будем обозначать $\frac{\xi_1+\dots+\xi_n}n = \ol\xi$. Тогда
\[
  \E f(\xi_i)\approx \frac{f(\xi_1) + \dots + f(\xi_n)}n.
\]
Здесь примерное равенство означает, что вероятность любой ошибки стремится к нулю.
\[
  \sigma^2 = \D\xi_i = \E(\xi_i-a)^2 \approx \frac1n\RY i1n (\xi_i -\ol\xi)^2.
\]

Пусть $a$ "--- измеряемая величина. Делаем $n$ измерений $x_1,x_2,\dots,x_n$. Тогда
\[
  a\approx \ol x = \frac1n \RY i1n x_i.
\]
Как тогда оценить $|\ol x - a|$?

Делаем предположение, что $x_i = a+ \delta_i$, причём $\delta_i$ "--- независимые одинаково распределённые случайные величины, для которых $\E\delta_i = 0$, то есть нет систематической ошибки. Удалять систематические ошибки эксперимента должен физик без всякой математики. А вот то, что это случайные величины, комментировать сложнее. Это же конкретные числа, вовсе не случайные. Здесь мы возносимся ввысь и говорим, что будем рассматривать $x_i$ как реализации $n$ независимых случайных величин. Если это случайность, то должен быть ансамбль опытов. $x_1,\dots,x_n$ "--- это не $n$ опытов, а один большой опыт. Зделали бы ещё опыт, были бы числа $y_1,\dots, y_n$, а ещё потом $z_1,\dots, z_n$. Этих чисел $y_i$ и $z_i$ у нас нет, но мы можем их выдумать. Если Бога нет, то следует его выдумать.

Наблюдаем $\ol x = a +\frac 1n\RY i1n \delta_i$. Значит, $\ol\delta \tever 0$. Закон больших чисел даёт надежду на то, что $\ol \delta$ будет чем-то вроде нуля с большой вероятностью, то есть в большинстве случаев. То есть среди ансамблей $x_i$, $y_i$, $z_i$, \ldotst{} мало случаев, когда $\ol\delta$ сильно отличается от нуля. Но почему именно наш ансамбль иксов оказался хорошим? Надо оценить, насколько большой может быть всё-таки $\ol\delta$.

Обозначим $\D\delta_i = \sigma^2$, $\sqrt{\D\delta_i} = \sigma.$ Пусть каким-то чудом $\sigma$ нам известна. Тогда
\[
  \D\ol \delta = \D\left(\frac{\delta_1+\dots+\delta_n}n\right) = \frac1{n^2} n\sigma^2 = \frac{\sigma^2}n.
\]
Дисперсия среднего арифметического в $n$ раз меньше дисперсии слагаемых. Стандартное отклонение "--- в $\sqrt{n}$ раз. Тогда
\[
  \P\bigg\{|\ol\delta|>\frac{3\sigma}{\sqrt n}\bigg\}\le \frac{\D\ol\delta}{9\frac{\sigma^2}n} = \frac19.
\]
Это если $\sigma$ нам известна. Считаем $S^2 = \frac1n\RY i1n(x_i-\ol x)^2$. Но наука в лице Гаусса сказал, что лучше использовать
\[
  s^2 = \frac1{n-1}\RY i1n(x_i - \ol x)^2\approx \sigma^2.
\]

Таким образом, сделав измерения $x_1,\dots,x_n$, вычислим $\ol x$ и $s^2 = \frac1{n-1}\RY i1n(x_i-\ol x)^2$. Тогда
\[
  |\ol x - a|\le \frac{3 s}{\sqrt{n}}
\]
На это можно надеяться.

Закон больших чисел мы в прошлый раз доказывали. Но то, что мы делаем сегодня, это, конечно, не математическая теорема. 

Пусть есть $\ol x$ и $\ol y$. Имеем $\E\ol x = a = \E \ol y$. Значит, (допустим, что всё-таки дисперсии разные)
\[
  \E(\ol x - \ol y) = 0,\quad \D(\ol x - \ol y) = \D\ol x + \D\ol y = \frac{\sigma_x^2}m + \frac{\sigma_y^2}n\approx \frac{s_x^2}m + \frac{s_y^2}n.
\]
Тогда давайте подберём $\e$, чтобы следующая вероятность была оценена сверху числом $\frac19$.
\[
  \P\big\{ |\ol x- \ol y|>\e\big\} \le \frac{\D(\ol x- \ol y)}{\e^2}= \frac19.
\]
Отсюда $\e = 3\sqrt{\frac{s_x^2}m + \frac{s_y^2}n}$. Мы можем и убрать оттуда тройку. Ввести величину $\gamma = \sqrt{\frac{s_x^2}m + \frac{s_y^2}n}$. Если $|\ol x - \ol y|\le \gamma$, хорошо. Это ещё один способ взглянуть на наши измерения.

Чтобы закончить введение в теорию вероятности, надо сделать ещё немного. Обозначение для распределения веротности
\[
  \begin{pmatrix}
    a_1 & a_2 &\dots\\
    p_1 & p_2 &\dots
  \end{pmatrix}
\]
годится не всегда.

\subsection{Предельная теорема Пуассона}
Мы с вами изучали испытания Бернулли. Пусть есть $n$ испытаний, $\mu$ "--- успехов, $p$ "--- вероятность успеха. Тогда
\[
  \P\{\mu = m\} = C_n^m p^m q^{n-m},\quad q = 1-p.
\]
На компьютере, конечно, это считать можно. Но вообще не особо наглядно получается задавать эту случайную величину таблицей.

Запишем схему серий: число испытаний, вероятность успеха, количество успехов
\[
\begin{matrix}
  n=1 & p_1 & \mu_1\\
  n=2 & p_2 & \mu_2\\
  \vdots & \vdots & \vdots\\
  n & p_n & \mu_n
\end{matrix}
\]
Пуассон утверждает, что удобно устремить $n p_n\to \lambda>0$, то есть $p_n = \frac\lambda n + o\left(\frac1n\right)$.
\begin{multline*}
  \P\{\mu = m\} = \C_n^m p_n^m(1-p_n)^{n-m} = 
  \frac{n(n-1)\dots(n-m+1)}{m!}\left(\frac\lambda n + o\left(\frac1n\right)\right)^n\left(1 - \frac\lambda n - o\left(\frac1n\right)\right)^{n-m} = \\=
  \frac{n(n-1)\dots(n-m+1)}{m!}\left(\frac{\lambda^m}{n^m} + \dots\right)e^{-\lambda} = \frac{\lambda^m}{m!}e^{-\lambda}.
\end{multline*}
Зададим случайную величину $\mu=0,1,2,\dots$, $\P\{\mu=m\} = \frac{\lambda^m}{m!}e^{-\lambda}$, она называется законом Пуассона. Легко посчитать
\[
  \E\mu \lambda,\quad \D\mu = \lambda.
\]

Нам нужен закон Пуассона, чтобы какие-то вероятности относительно легче считались. Давайте применим закон Пуассона.
\subsection{Проверка статистических гипотез}
Статистическая гипотеза "--- любое положение, позволяющее считать какие-то вероятности.

Имеется множество $\Omega$ событий. Опыт заканчивается результатом $\omega\in \Omega$. То, чем опыт кончается, нас не волнует. Пусть $X=\{x\}$ "--- множество наблюдаемых вещей. Есть какое-то отображение $\Omega\to X$. Гипотеза задаёт правило, по которому считаются вероятности вида
\[
 \P\{x\in A\subset X\mid H\}.
\]
Как проверить, что эта гипотеза верна. Выберем $S\subset X$, для которого $\P\{x\in S\mid H\}\le \alpha$. Число $\alpha$ называют в этом контексте уровнем значимости. Возможны случаи
\begin{roItems}
  \item $x\in S$. Тогда гипотеза $H$ отклоняется на уровне значимости $\alpha$.
  \item $x\not\in S$. Тогда гипотеза $H$ не отклоняется на уровне значимости $\alpha$.
\end{roItems}

Рассмотрим пример. На $1\,000$ булочек уходит $10\,000$ изюминок. Производитель так объявил, это его реклама. Но работники фабрики могут иметь способ воровать изюминки. Нужно найти способ понять, воруют или нет. Обобзначим $H_a$ для $0\le a\le 1$ гипотезу «доля $a$ украдена».

Возьмём булочку, обозначим $\mu$ "--- число изюменок в ней. Пусть у наc $10\,000$ испытаний, то есть перебираем изюминки. Вероятность, что данная изюминка попала в данную булочку $p = \frac1{1\,000}$. Числа большие, это случай пуасона: $\lambda = np = 10(1-a)$.

Будем брать булочки. Пусть $x$ "--- число изюминок в будлочке. Пусть $S=\{x=0\}$. Если $H_0$ верна, то вероятность, распределённая по Пуассону,
\[
  \P\{x=0\}=\approx 0.5\cdot 10^{-4}.
\]
Пусть всякий человек, купивший булочку без изюма, пишет жалобу. А принимающий такую жалобу тут же закрывает фикрму. Вероятность, что жалобу напишет конкретный человек, $10^{-4}$. А вероятность, что напишут хотя бы одну жалобу, $0.05$ (ну тысяча булочек). Но булки-то пекутся каждый день. В году вероятность, что не будет ни одной жалобы $(1-0.05)^{365} = 0$ (ну это в общем нуль). Булочки печь будет уже некому.