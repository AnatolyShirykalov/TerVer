\section{Теория оценок}

 Есть измерения $x_1,\dots,x_n$. Предположим, что мы знаем какой характер у совместной плотности $p(x_1,\dots,x_n;\theta)$ случайных величин, реализацией которых мы предполагаем выборку, но не знаем один параметр этой плотности. Допустим, мы предположил, что закон нормальный у одного $x$, тогда
\[
  p(x) = \frac1{\sigma\sqrt{2\pi}} \exp\left\{ -\frac{(x-a)^2}{2\sigma^2} \right\},\quad \theta = \{a,\sigma\}.
\]

А что такое плотность. $\P(\xi\in B) = \int\limits_B p_\xi(x)\,dx$, где $\xi = \xi(\omega)\in X$.

Пусть $X=\{0,1,2,\dots\}$ и  $\P(\xi=x) = \frac{\lambda^x}{x!}e^{-\lambda}$. Какая здесь плотность? Какая здесь мера? На множестве $X$ можно поместить в каждой точке массу единицу. Тогда будет интаграл по этой мере-массе.

Итого
\[
  \int\limits_B p(x,\theta)\,dx=\P(B).
\]
Это такое обобщение понятия плотности и вероятности.

Итак есть наблюдения $x_1,\dots,x_n$. Мы уверены, что некая формула, зависящая от $x$ и $\theta$, даёт нам разумное описание тех вероятностей, которые нам дают $x_1,\dots,x_n$.

Оценкой называют любую функцию от измерения
\[
  \hat\theta = \phi(x_1,\dots,x_n).
\]

У нас постоянно будут возникать выражения $\log p(x,\theta)$. Это натуральный логарифм, просто следуем обозначениям учебника Себастьянова. Однако $p(x,\theta)\ge 0$. Равняться нулю может.  Для показательного закона
\[
  p(x,\theta) = \theta e^{-\theta x} J_{x\ge 0}.
\]
Надо понять, какой смысл за этим будет стоять.

Наблюдения $(x_1,\dots,x_n)\in X$. Выделяем в множестве $X$ такую область. Выделяем такие $x\in X$, где плотность $p(x_1,\dots,x_n,\theta)>0$. В дальнейшем эта область не обозначается никак. Надо экономить буквы. Будем далее вообще предполагать, что эта область не зависит от $\theta$. Для равномерного распределения это не так
\[
  p(x,\theta) = \frac1\theta J_{x\in[0,\theta]}.
\]
Тут область, где $p>0$, зависит от $\theta$.  Для $\theta$ есть интуитивная оценка $\max\{x_1,\dots,x_n\}$. Она будет точнее, чем то, что я далее буду рассказывать.

Интеграл по области $\{p(x,\theta)>0\}$ $\int\dots\,dx$ будет неопределённым. Мы будем писать без пределов интеграл, который берётся по всей области $\{p>0\}$.

Мы сейчас выведем, точнее всего не может быть оценка. При дополнительных предположениях о гладкости плотности по оцениваемому параметру. Имеем равенство $\int p(x,\theta)\,dx =1$. Дифференцируем его
\[
  \DP{ }\theta \int p(x,\theta)\,dx = \int \CP p\theta\,dx = 0.
\]
Если бы область зависела от $\theta$, то производную мы бы так не протащили.

Поскольку оценка в конечном итоге получается случайной величиной. Отсюда можно выписать основные характеристики оценки.
\subsection{Мат ожидание оценки}
 $g(\theta) = \E \hat\theta = \E \phi(x) = \int\limits_X \phi(x) p(x,\theta)\,dx$. Здесь до того, как писали интеграл, под $x$ понималась случайная величина, а под интегралом это уже переменная интегрирования. Далее выделим те иксы, которые дают плотность ноль. Ничего не изменится, если их убрать. И мы договорились ничего не подписывать, когда интеграл берётся по множеству $p>0$.
\[
  g(\theta) = \E(\hat\theta) = \E\phi(x) = \int\phi(x) p(x,\theta)\,dx = \int \phi p\,dx.
\]

Мы надеемся, что $g(\theta) \approx \theta$, хотя бы для большого числа наблюдений.

\subsection{Дисперсия оценки}
Давайте напишем, что такое производная $g$
\[
  g'(\theta) = \int \phi\CP p\theta\,dx.
\]
Пришло время воспользоваться понятием логарифма. Вводится величина, называемая информацией Фишера. Обозначается $J(\theta) = \E \left(\CP{ }\theta\log p(x,\theta)\right)^2$. Выпишем, что получится
\[
  J(\theta) = \int \left( \frac{p'_\theta}{p} \right)^2p\,dx
\]

Если $g(\theta)=0$, оценка называется несмещённой.

Посмотрим снова на производную
\[
  g'(\theta) = \int \phi \CP p\theta \,dx = \int\phi \CP{\log p}\theta p(x,\theta)\,dx.
\]

Но у нас есть равенство
\[
  0 = \int \CP p\theta \,dx = \int \CP{\log p}\theta p(x,\theta)\,dx.
\]
Умножим его на $g(\theta)$ и привавим к производной.
\[
  g'(\theta) =\int (\phi - g)\CP{\log p}{\theta}p(x,\theta)\,dx. 
\]
Ну а это скалярное произведение в $L_2$ с весом $p(x,\theta)$. Значит,
\[
  \big[g'(\theta)\big]^2\le \int \big[\phi(x)-g(x)\big]^2p\,dx \cdot J(\theta).
\]
Второй множитель оказался информацией Фишера. А первый множитель есть дисперсия. Получается следующее неравенство
\[
\big[  g'(\theta)\big]^2\le \D \hat\theta \cdot J(\theta).
\]
Или, эквивалентно, $\D\hat\theta\ge \frac{\big[g'(\theta)]^2}{J(\theta)}$. Это знаменитое неравенство Рао"--~Крамера.

Вот мы хотим $g(\theta)\approx \theta$. Бывает так, что функции близки, а производные у них не близки, но в приложениях этого не бывает. Поэтому если $g'(\theta)$, то
\[
  \D\hat\theta \ge \frac1{J(\theta)}.
\]

Вот так получается наилучшая точность для оценки.




Теперь возьмём вторую производную
\[
  \CP{^2}{\theta^2}\log p = \CP{ }\theta\left( \frac{p'_\theta}{p} \right) = \frac{p''_{\theta\theta} p - (p'_\theta)^2}{p^2} = \frac{p''_{\theta\theta}}p - \left( \frac{p'_\theta}{p} \right)^2.
\]
Однако $\int p''_{\theta\theta}\,dx = 0$, поэтому информацию Фишера ещё можем написать
\[
  J(\theta) = -\E \CP{^2\log p}{\theta^2}.
\]

Есть у нас наблюдения $x_1,\dots,x_n$, запишем плотность, будто все распределены нормально
\[
  p(x,a,\sigma) = \left( \frac{1}{\sigma\sqrt{2\pi}} \right)^n \exp\left\{ -\frac{\sum(x_i-a)^2}{2 \sigma^2} \right\}.
\]
В качестве оценок возьмём $\hat a = \frac1n \sum s_i$, $\D \hat a = \frac{\sigma^2}n$. Тогда информаци Фишера
\[
   J = -\E\left\{ C - \frac{ \sum (x_i-a)^2}{2\sigma^2} \right\}''_{aa} = \frac{n\sigma^2}.
\]
А у нас есть оценка $\D\hat a\ge \frac{\sigma^2}n$. Ну значит предложенная оценка с помощью среднего является минимизирующей дисперсию.

Оценка с минимальной дисперсией называется \textbf{эффективный}.

\section{Метод максимального правдоподобия}
Полное доказательство сегодня не успею. Обсудим, что это вообще.

Пусть есть $x_1,\dots,x_n$. Пусть они описываются плотностью $p(x,\theta)$. Подставим наблюдение в плотность, останется только функция от одного $\theta$. Обратим это в максимум по $\theta$.
\[
  p(x,\theta)\to\max\limits_\theta.
\]
Полученный $\arg\max\limits_\theta p(x,\theta) = \hat\theta(x_1,\dots,x_n)$. Это некоторая оценка.

Пусть плотность одного наблюдения $p(x_i,\theta)$. Тогда плотность совокупности независимых наблюдений есть произведение.
\[
  \prod\limits_{i=1}^n p(x_i,\theta) = L(\theta).
\]
Это называется функцией правдоподобия. Пусть $\theta_0$ "--- истинное значение оцениваемого параметра. Возьмём логаримф от функции правдоподобия
\[
  \log L(\theta) = \RY i1n\log p(x_i,\theta). 
\]
И возьмём производную по $\theta$
\[
  \frac1n\CP{ }\theta \log L(\theta) = \frac1n \RY i1n\CP{ }\theta\log p(x_i,\theta) = 0.
\]
Распишем по формуле Тейлора то, что здесь у нас написано.
\[
  \frac1n\CP{ }\theta\log L(\theta) = \frac 1n \RY i1n\CP{ }\theta\log p(x_i,\theta) + (\theta-\theta_0)\frac1n\RY i1n\CP{^2}{\theta^2}\log p(x_i,\theta) + 
  \frac12 (\theta-\theta_0)^2 \frac1n \RY i1n\CP{^3}{\theta^3}\log p(x_i,\theta_i) = 0.
\]
Мы допустим, что $\left|\CP{^3}{\theta^3}\log p(x,\theta)\right|\le H(x)$ так, что если мы подставим вместо $x$ случайную величину и посчитали бы мат ожидание, то получилась бы ограниченная величина. Тогда получится, что остаточный член мал, то есть $\E H(x)\le M$.

Дело в том, что $\E\CP{ }\theta\log p(x_i,\theta) = 0$, а дисперсия такой суммы равна суммы дисперсий.
Берём мат ожидание. Получаем по втором слагаемом информацию Фишера. ПОлучается, что если только сумму первых двух членов приравнять к нулю, то получается, что
Дисперсия первого слагаемого порядка $J_1/n$, а мат ожидание ноль. Значит, сама величина будет порядка $\sqrt{J_1/n}$.
\[
  \Til\theta - \theta_0\sim\frac1{\sqrt{n}}
\]
А что будет с учётом полного уравнения $(\Til \theta-\theta_0)^2 \sim \frac1n\cdot \const$. Понятно, что меняя $\hat\theta$ на величину порядка $\frac1n$, мы можем получить решение полного уравнения. Это из последнего слагаемого.

А со вторым слагаемым мы решим. Это я просто хотел показать идею. Полное доказательство в следующий раз.
