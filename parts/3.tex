\section{Лекция 3}
Чему мы научились за первые две лекции. Есть множество элементарных событий $\Omega = \{\omega\}$, на нём задана неотрицательная функция $\P(\omega)$, считаем $\P(A) = \sum\limits_{\omega\in A} \P(\omega)$. Тогда $\P(A+B) = \P(A) + \P(B)$, для непересекающихся событий.

Мы разбирали пример, когда есть множество людей. Вероятность наличия болезни $\P(\text{Б})$, $\P(\text{успешного лечения}) = 0.95$ "--- найти теоретически невозможно, только эмпирическими испытаниями. Было $N$ больных, $n$ неудач, оказалось, что $n/N = 0.95$. Дальнейшие рассуждения опирались на то, что вероятность выздороветь одинаковая. Но может быть и не так. Может быть так, что для 95\% больных $\P(\text{успеха}) = 1$, для остальных "--- ноль. Отличить эти ситуации невозможно. Что мы получим при втором варианте? Те, кто выздоровел не будет жаловаться, и за счёт этого будет складываться соответствующее общественное мнение.

Надо изучать вопрос, насколько мы можем ошибиться, вычисляя предполагаемое значение, по отношению к истинному значению.

\subsection{Случайные величины}
Случайной величиной называется любая функция $\xi = \xi(\omega)$, определённая на $\Omega$. Она может принимать действительные значения или комплексные и даже кватернионные, не важно.
Напишем значения, которые принимает эта функция, строку, а под этими значениями вероятность того, что она их достигнет (вероятность прообраза). Такая таблица
\[
  \begin{pmatrix}
  a_1 & a_2 &\dots  & a_n &\dots\\
  p_1 & p_2 &\dots & p_n & \dots
\end{pmatrix},
\]
где $p_i = \P(\xi=a_i) = \P\big\{\omega\colon \xi(\omega) = a_i\big\} = \sum\limits_{\omega\colon \xi(\omega)=a_i}\P(\omega)$, называется распределеним вероятности случайной величины $\xi$.

Хороша, когда есть $\Omega$ и все $\xi(\omega)$. Но часто бывает так, что $\xi(\omega)$ трудно задать на всех $\omega$. Можно перейти от такой таблицы к некоторым параметрам, характеризующим случайную величину.

Одна из таких величин называется \textit{математическим ожиданием} $\E$ от слова expectation. Определяется $\E$ следующим образом
\[
  \E\xi = \sum\limits_{\omega\in\Omega}\xi(\omega)\P(\omega)
\]
для тех случайных величин, для которых сумма абсолютно сходится. Имеются следующие свойства
\[
  \E (c\xi) = c\E\xi,\quad \E(\xi + \eta) = \E\xi + \E\eta.
\]
Таким образом, $\E$ "--- это некоторый линейный функционал.
\begin{Lem}
  Пусть есть $\xi(\omega)$ "--- некоторая функция на $\Omega$. Пусть также есть $\eta = f(\xi)$. Тогда можно образовать суперпозицию $\eta(\omega) = f\big(\xi(\omega)\big)$. Лемма утверждает, что
\[
  \xi f(\xi) = \sum\limits_a f(a_i) p_i = \sum\limits_{a_i} f(a_i) \P\{\xi = a_i\}.
\]
В частности, если $f(x) = x$, то $f(\xi) = \xi$ и $\E \xi = \sum\limits_{a_i} a_i p_i$.
\end{Lem}
\begin{Proof}
  Имеем
\[
  \E f = \sum\limits_{\omega\in\Omega} f\big(\xi(\omega)\big) \P(\omega) = 
  \sum\limits_{\{a_i\}} \bigg(\sum\limits_{\omega\colon \xi(\omega) = a_i} f(a_i)\P(\omega)\bigg) =
\]
В каждом слагаемом внутренней суммы $a_i$ фиксированно, значит, можно вынести.
\[
=\sum\limits_{a_i} f(a_i) \P\{\xi = a_i\}.
\]
Что и требовалось доказать.
\end{Proof}

А не было ли что-то похожего в книгах по механике? Если я в точку $a_i$ на прямой помещу массу $p_i$, то $\sum a_i p_i$ есть центр массы. Но раз есть центр массы, то есть и момент инерции. И он есть и имеет важнейшую роль в теории вероятности.

\subsection{Дисперсия}
Ужас в том, что хоть мат. ожидание по английски expectation, но дисперсия "--- variance.
\begin{Def}
Дисперсией случайной величины $\xi$ называется
\[
  \D\xi(\xi - \E \xi)^2 = \E (\xi - \E\xi)^2 = \sum\limits_{a_i}(a_i - E\xi)^2p_i.
\]
\end{Def}

В механике такая величина называется моментом инерции. А такую величину в механике видали?
\[
\sum\limits_{a_i}(a_i - E\xi)^3p_i
\]
Я не видал.

\subsection{Независимые случайные величины}
У нас есть определение независимых событий. Там фигурировало понятие совместного наступления событий. Но случайные величины не наступают совместно и вообще не наступают. Нужно формулировать как-то по-другому.

Пусть $A,B$ "--- числовые множества, $\xi,\eta$ "--- случайные величины. Выделим таким события
\[
  \{\xi\in A\} = \big\{\omega\colon \xi(\omega)\in A\big\},\quad
  \{\eta\in A\} = \big\{\omega\colon \eta(\omega)\in B\big\}.
\]
Теперь важнейшее определение.
\begin{Def}
  Если для любых числовых множеств $A$ и $B$ события $\{\xi\in A\}$  и $\{\eta\in B\}$ независимы, то есть (запятая означает «и», то есть пересечение)
\[
  \P\{\xi\in A,\eta\in B\} = \P\{\xi\in A\} \P\{\eta\in B\}.
\]
\end{Def}

Откуда же таким взяться. Если взять $\Omega = \Omega^{(1)} \times \Omega^{(2)} = \{\omega^{(1)},\omega^{(2)}\}$ и такие случайные величины, которые формально зависят от всего $\omega = (\omega^{(1)},\omega^{(2)})$, но по факту каждая только от одной компонетны, то есть $\xi = \xi(\omega^{(1)})$, $\eta = \eta(\omega^{(2)})$, то можно получить независимость. Пусть $\xi(\omega) = a_i$, $\eta(\omega) = b_j$. Вполне достаточно, чтобы такие события были независимы. Действительно,
\[
  \{\xi\in A\} = \bigcup\limits_{a_i\in A}\{\xi = a_i\},\quad
  \{\eta\in B\} = \bigcup\limits_{b_j\in B}\{\eta = b_j\}.
\]
И тогда вероятность $\P\{\xi\in A,\eta\in B\} = \sum\limits_{\substack{a_i\in A\\ b_j\in B}} \P\{\xi = a_i,\eta = b_j\}$. Абсолютная сходимость, которая есть из неотрицательности $\P$, гарантирует, что сумма произведений есть произведение сумм.

Пусть случайные величины $\xi$ и $\eta$ независимы. Построим новые случайные величины $f(\xi)$ и $g(\eta)$. Пусть $f(\xi\in A$ (эф от кси попало в $A$), это значит, что $\xi\in f^{-1}(A)$. Не факт, что новые случайные величины будет независимы.

Давайте выпишем: события $A$ и $B$ независимы, если $\P(AB) = \P(A)\P(B)$.
\begin{Lem}
Пусть $\xi,\eta$ независимы, а $\E\xi$ и $\E\eta$ существуют. Тогда
\[
  \E(\xi\eta) = \E \xi\eta = \E\xi\cdot \E\eta.
\]
\end{Lem}
\begin{Proof}
Вспомним, что математическое ожидание можно написать следующим образом
\[
  \E f(\xi) = \sum\limits_{a_i} f(a_i)\P\{\xi=a_i\}.
\]
При замене $a_i$ на векторы, пары чисел, что угодно, ничего в доказательстве меняться не будет.

Возьмём функцию $f(a,b) = ab$. Тогда
\[
  \E\xi\eta = \E f(\xi,\eta) = \sum\limits_{(a_i,b_j)} a_ib_j\P\{\xi=a_i,\eta=b_j\} =
  \sum\limits_{(a_i,b_j)} \P\{\xi = a_i\}\P\{\eta=b_j\}=
  \sum\limits_{a_i} a_i\P\{\xi = a_i\} \sum\limits_{b_j}b_j\P\{\eta = b_j\}.
\]
Математик пришёл бы в ужас, что мы не наложили каких-то ограничений на величины. Но мы ответим: наложили. Мы сказали, что $\E\xi$ и $\E\eta$ существуют, а поэтому суммы сходятся абсолютно.
\end{Proof}

А что будет с дисперсией любых двух случайных величин, не обязательно независимых.
\begin{multline*}
  \D(\xi + \eta) = \E\Big\{\big((\xi+\eta) - \E(\xi+\eta)\big)^2\Big\} = 
  \E \big( (\xi - \E\xi) + (\eta - \E\eta)\big)^2 = \\=
  \E \big\{(\xi-E\xi)^2 + (\eta - \E\eta)^2 + 2(\xi - \E\xi)(\eta - \E\eta)\big\}=
  \D\xi + \D\eta +  2\E(\xi-\E\xi)(\eta - \E\eta).
\end{multline*}

Последнее слагаемое нам не нравится, но никак от него не избавиться. Поэтому придётся ввести новое понятие.

\begin{Def}
  Величина
\[
  \cov(\xi,\eta) = \E(\xi - \E\xi)(\eta - \E\eta)
\]
называется ковариацией.
\end{Def}

Теперь можем записать дисперсию суммы
\[
  \D(\xi_1+\dots+\xi_n) = \sum\limits_i \D\xi_i + \sum\limits_{i\ne j}\cov(\xi_i,\xi_j).
\]

А если $\xi$ и $\eta$ независимы, то
\[
  \cov(\xi,\eta) = \E(\xi - \E\xi)\cdot \E(\eta - \E\eta) = 0.
\]

Для независимых случайных величин, у которых существует дисперсия, дисперсия суммы есть сумма дисперсий.

\subsection{Неравенство Чебышёва}
Понятно, что дисперсия как-то измеряет отклонение случайной величины от математического ожидания. Чебышёв обнаружил полезное неравенство для тех случаев, когда дисперсия мала.
\[
  \P\big\{|\xi - \E\xi|\ge \e\big\}\le \frac{\D\xi}{\e^2}.
\]


Давайте его докажем.
\begin{Proof}
Обычно так доказывают.
\[
  \D\xi = \sum\limits_{a_i} (a_i - \E\xi)^2 \P\{\xi = a_i\}\ge
\]
мы эту сумму уменьшим, если складывать не по всем $a_i$, а по некоторым
\[
  \ge \sum\limits_{a_i\colon |a_i - \E\xi|\ge \e}(a_i - \E\xi)^2\P\{\xi = a_i\}\ge 
  \sum\limits_{a_i\colon |a_i - \E\xi|\ge \e} \e^2\P\{\xi = a_i\} = 
  \e^2 \P\big\{|\xi = \E\xi|\ge \e\big\}.
\]
\end{Proof}

\subsection{Закон больших чисел в форме Чебышёва}
Пусть имеется набор попарно независимых случайных величин $\xi_1,\xi_2,\dots,\xi_n,\dots$, причём пусть $\D \xi_i<C$. Тогда рассмотрим
\[
  \P\Bigg\{\bigg|\frac{\xi_1 + \dots + \xi_n}{n} - \frac{\E\xi_1 + \E \xi_2+\dots + \E\xi_n}{n}\bigg|\ge \e\Bigg\}\te 0.
\]

По неравенству Чебышёва
\[
  \P\big\{|\xi_n - \E\xi_n|\ge\e\big\} \le \frac{\D\xi_n}{\e^2}
\]
достаточно доказать, что $\D\xi_n\te 0$.
\[
  \D \bigg\{\frac{\xi_1+\dots+\xi_n}{n} \bigg\} \le \frac1{n^2}C n = \frac Cn\te 0.
\]

Пусть мы хотим узнать велицину $a$. Мы знаем $x_i = a+ \delta_i$ "--- показания приборов. Пусть также $\delta_1,\delta_2,\dots,\delta_n$ "--- независимые одинаково распределённые случайные величины, причём $\E\delta_i = 0$. Пусть $\sigma^2= \D \delta_i$. Определим величину
\[
  \ol x = \frac1n \sum x_i = a+ \frac1n \RY i1n\delta_i.
\]

Второе слагаемое имеет дисперсию $\frac{\sigma^2}n$.

Поскольку $\D\xi = \E(\xi - \E\xi)^2$, то можно оценить порядок отклонения $|\xi - \E\xi|\sim \sqrt{\D\xi}$. А если $\E\xi_i=a$, то модуль разности
\[
  \bigg|\frac{\xi_1 + \dots +\xi_n}{n} - a\bigg|
\]
мал, если $n$ велико. Откуда взять $\sigma^2$? А вот 
\[
  S^2 = \frac1n\sum (x_i-\ol x)^2\approx \sigma^2.
\]
Часто также используют
\[
  s^2 = \frac1{n-1}\sum(x_i-\ol x)^2.
\]
Так как оценить отклонение. А вот так
\[
  \frac\sigma{\sqrt n}\approx \frac s{\sqrt{n}}.
\]
